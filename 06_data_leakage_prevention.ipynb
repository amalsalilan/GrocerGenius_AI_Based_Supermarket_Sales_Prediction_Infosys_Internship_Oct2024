{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importing necessary libraries for data preprocessing, modeling, and evaluation.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocessing function to handle missing values, derive new features, and transform data.\n",
    "def preprocess_data(data):\n",
    "    data['Item_Weight'] = data['Item_Weight'].fillna(data.groupby('Item_Type')['Item_Weight'].transform('mean'))\n",
    "    data['Outlet_Size'] = data['Outlet_Size'].fillna(data['Outlet_Size'].mode()[0])\n",
    "\n",
    "    # Feature derivation\n",
    "    data['Outlet_Age'] = 2024 - data['Outlet_Establishment_Year']\n",
    "    data['Price_Per_Unit_Weight'] = data['Item_MRP'] / data['Item_Weight']\n",
    "    \n",
    "    # Simplifying Item_Fat_Content\n",
    "    data['Item_Fat_Content'] = data['Item_Fat_Content'].replace({'LF': 'Low Fat', 'low fat': 'Low Fat', 'reg': 'Regular'})\n",
    "\n",
    "    # Log Transformation for Item Visibility\n",
    "    data['Item_Visibility_Log'] = np.log1p(data['Item_Visibility'])\n",
    "    \n",
    "    # MRP Categorization\n",
    "    min_value = data['Item_MRP'].min()\n",
    "    max_value = data['Item_MRP'].max()\n",
    "    range_value = max_value - min_value\n",
    "    data['MRP_Tier'] = data['Item_MRP'].apply(lambda x: 'Low' if x <= min_value + 0.33 * range_value else\n",
    "                                              'Medium' if x <= min_value + 0.66 * range_value else 'High')\n",
    "\n",
    "    return data\n",
    "\n",
    "# Step 2: Loading data and splitting into training and testing sets.\n",
    "data = pd.read_csv('C:\\\\Users\\\\Kamlesh P Panchal\\\\Documents\\\\Infosys Internship\\\\train_og\\\\Train.csv')\n",
    "training_data, testing_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Preprocessing training and testing datasets.\n",
    "training_data = preprocess_data(training_data)\n",
    "testing_data = preprocess_data(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Identifier</th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Item_Fat_Content</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_Type</th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Outlet_Identifier</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "      <th>Outlet_Size</th>\n",
       "      <th>Outlet_Location_Type</th>\n",
       "      <th>Outlet_Type</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FDA15</td>\n",
       "      <td>9.3</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.016047</td>\n",
       "      <td>Dairy</td>\n",
       "      <td>249.8092</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 1</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>3735.138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Item_Identifier  Item_Weight Item_Fat_Content  Item_Visibility Item_Type  \\\n",
       "0           FDA15          9.3          Low Fat         0.016047     Dairy   \n",
       "\n",
       "   Item_MRP Outlet_Identifier  Outlet_Establishment_Year Outlet_Size  \\\n",
       "0  249.8092            OUT049                       1999      Medium   \n",
       "\n",
       "  Outlet_Location_Type        Outlet_Type  Item_Outlet_Sales  \n",
       "0               Tier 1  Supermarket Type1           3735.138  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Function for encoding categorical and scaling numerical features.\n",
    "\n",
    "def encode_data(data, is_training=True, encoders=None):\n",
    "    # feature groups\n",
    "    numeric_features = ['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Age', 'Price_Per_Unit_Weight']\n",
    "    ordinal_features = ['Outlet_Size', 'MRP_Tier']\n",
    "    nominal_features = ['Item_Fat_Content', 'Outlet_Location_Type', 'Outlet_Type', 'Item_Type']\n",
    "    label_features = ['Item_Identifier', 'Outlet_Identifier']\n",
    "\n",
    "    # Filtering the columns existing in the dataset\n",
    "    numeric_features = [col for col in numeric_features if col in data.columns]\n",
    "    ordinal_features = [col for col in ordinal_features if col in data.columns]\n",
    "    nominal_features = [col for col in nominal_features if col in data.columns]\n",
    "    label_features = [col for col in label_features if col in data.columns]\n",
    "\n",
    "    if is_training:\n",
    "        # Fitting encoders on training data\n",
    "        encoders = {\n",
    "            'ordinal': OrdinalEncoder().fit(data[ordinal_features]) if ordinal_features else None,\n",
    "            'nominal': OneHotEncoder(sparse_output=False, drop='first').fit(data[nominal_features]) if nominal_features else None,\n",
    "            'label': {col: LabelEncoder().fit(data[col]) for col in label_features},\n",
    "            'scaler': StandardScaler().fit(data[numeric_features]) if numeric_features else None\n",
    "        }\n",
    "\n",
    "    # Applying transformations\n",
    "    if numeric_features:\n",
    "        data[numeric_features] = encoders['scaler'].transform(data[numeric_features])\n",
    "    if ordinal_features:\n",
    "        data[ordinal_features] = encoders['ordinal'].transform(data[ordinal_features])\n",
    "    if nominal_features:\n",
    "        nominal_encoded = encoders['nominal'].transform(data[nominal_features])\n",
    "        nominal_cols = encoders['nominal'].get_feature_names_out(nominal_features)\n",
    "        data = pd.concat([data.reset_index(drop=True), pd.DataFrame(nominal_encoded, columns=nominal_cols)], axis=1)\n",
    "        data.drop(columns=nominal_features, inplace=True)\n",
    "\n",
    "    # Label encode ID columns\n",
    "    for label_feature in label_features:\n",
    "        le = encoders['label'][label_feature]\n",
    "        data[label_feature] = data[label_feature].map(\n",
    "            lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
    "        )\n",
    "        \n",
    "    # Saving encoders to reuse them for new data.\n",
    "    joblib.dump(encoders['ordinal'], 'ordinal_encoder.pkl')\n",
    "    joblib.dump(encoders['nominal'], 'onehot_encoder.pkl')\n",
    "    joblib.dump(encoders['scaler'], 'standard_scaler.pkl')\n",
    "\n",
    "    # Save label encoders for ID features\n",
    "    for label_feature in label_features:\n",
    "        joblib.dump(encoders['label'][label_feature], f'{label_feature}_label_encoder.pkl')\n",
    "\n",
    "    return (data, encoders) if is_training else data\n",
    "\n",
    "# Step 4: Encode training and testing data\n",
    "training_data, encoders = encode_data(training_data, is_training=True)  # Encoding on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading encoders and encoding testing data.   \n",
    "ordinal_encoder = joblib.load('ordinal_encoder.pkl')\n",
    "onehot_encoder = joblib.load('onehot_encoder.pkl')\n",
    "scaler = joblib.load('standard_scaler.pkl')\n",
    "\n",
    "item_identifier_encoder = joblib.load('Item_Identifier_label_encoder.pkl')\n",
    "outlet_identifier_encoder = joblib.load('Outlet_Identifier_label_encoder.pkl')\n",
    "\n",
    "testing_data = encode_data(testing_data, is_training=False, encoders=encoders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Identifier                      int32\n",
       "Item_Weight                        float64\n",
       "Item_Visibility                    float64\n",
       "Item_MRP                           float64\n",
       "Outlet_Identifier                    int32\n",
       "Outlet_Establishment_Year            int64\n",
       "Outlet_Size                        float64\n",
       "Item_Outlet_Sales                  float64\n",
       "Outlet_Age                         float64\n",
       "Price_Per_Unit_Weight              float64\n",
       "Item_Visibility_Log                float64\n",
       "MRP_Tier                           float64\n",
       "Item_Fat_Content_Regular           float64\n",
       "Outlet_Location_Type_Tier 2        float64\n",
       "Outlet_Location_Type_Tier 3        float64\n",
       "Outlet_Type_Supermarket Type1      float64\n",
       "Outlet_Type_Supermarket Type2      float64\n",
       "Outlet_Type_Supermarket Type3      float64\n",
       "Item_Type_Breads                   float64\n",
       "Item_Type_Breakfast                float64\n",
       "Item_Type_Canned                   float64\n",
       "Item_Type_Dairy                    float64\n",
       "Item_Type_Frozen Foods             float64\n",
       "Item_Type_Fruits and Vegetables    float64\n",
       "Item_Type_Hard Drinks              float64\n",
       "Item_Type_Health and Hygiene       float64\n",
       "Item_Type_Household                float64\n",
       "Item_Type_Meat                     float64\n",
       "Item_Type_Others                   float64\n",
       "Item_Type_Seafood                  float64\n",
       "Item_Type_Snack Foods              float64\n",
       "Item_Type_Soft Drinks              float64\n",
       "Item_Type_Starchy Foods            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model  Train R²   Test R²\n",
      "0  Linear Regression  0.559156  0.577484\n",
      "1      Random Forest  0.936858  0.563029\n",
      "2            XGBoost  0.884042  0.507696\n",
      "3              Lasso  0.558769  0.578327\n",
      "4              Ridge  0.559019  0.577875\n",
      "5  Gradient Boosting  0.632934  0.606336\n",
      "6           AdaBoost  0.481662  0.470757\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Splitting datasets into features (X) and target variable (y).\n",
    "\n",
    "X_train = training_data.drop('Item_Outlet_Sales', axis=1)\n",
    "y_train = training_data['Item_Outlet_Sales']\n",
    "X_test = testing_data.drop('Item_Outlet_Sales', axis=1)\n",
    "y_test = testing_data['Item_Outlet_Sales']\n",
    "\n",
    "# Step 7: Training models and evaluating performance.\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'XGBoost': XGBRegressor(),\n",
    "    'Lasso': Lasso(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(),\n",
    "    'AdaBoost': AdaBoostRegressor()\n",
    "}\n",
    "\n",
    "results = []\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    train_r2 = r2_score(y_train, model.predict(X_train))\n",
    "    test_r2 = r2_score(y_test, model.predict(X_test))\n",
    "    results.append({'Model': model_name, 'Train R²': train_r2, 'Test R²': test_r2})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Linear Regression...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Tuning Random Forest...\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Tuning XGBoost...\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Tuning Lasso...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Tuning Ridge...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Tuning Gradient Boosting...\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Tuning AdaBoost...\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "               Model                                        Best Params  \\\n",
      "0  Linear Regression                                                 {}   \n",
      "1      Random Forest  {'max_depth': 10, 'min_samples_leaf': 2, 'min_...   \n",
      "2            XGBoost  {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...   \n",
      "3              Lasso                     {'alpha': 1, 'max_iter': 1000}   \n",
      "4              Ridge                                    {'alpha': 0.01}   \n",
      "5  Gradient Boosting  {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...   \n",
      "6           AdaBoost         {'learning_rate': 0.1, 'n_estimators': 50}   \n",
      "\n",
      "   Train R²   Test R²  \n",
      "0  0.559156  0.577484  \n",
      "1  0.714706  0.608197  \n",
      "2  0.625660  0.606636  \n",
      "3  0.558769  0.578327  \n",
      "4  0.559138  0.577704  \n",
      "5  0.612761  0.612057  \n",
      "6  0.513441  0.508107  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'Linear Regression': {},  # No hyperparameters for tuning in sklearn's LinearRegression\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': [0.01, 0.1, 1, 10],\n",
    "        'max_iter': [1000, 2000, 5000]\n",
    "    },\n",
    "    'Ridge': {\n",
    "        'alpha': [0.01, 0.1, 1, 10]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Dictionary to store best models and results\n",
    "best_models = {}\n",
    "grid_search_results = []\n",
    "\n",
    "# Perform GridSearchCV for each model\n",
    "for model_name, param_grid in param_grids.items():\n",
    "    print(f\"Tuning {model_name}...\")\n",
    "    model = models[model_name]\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model and parameters\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Evaluate the model\n",
    "    train_r2 = r2_score(y_train, best_model.predict(X_train))\n",
    "    test_r2 = r2_score(y_test, best_model.predict(X_test))\n",
    "    \n",
    "    # Store the results\n",
    "    best_models[model_name] = best_model\n",
    "    grid_search_results.append({'Model': model_name, 'Best Params': best_params, 'Train R²': train_r2, 'Test R²': test_r2})\n",
    "\n",
    "# Convert results to a DataFrame for better visualization\n",
    "grid_search_results_df = pd.DataFrame(grid_search_results)\n",
    "print(grid_search_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model  Train R²   Test R²\n",
      "0  Linear Regression  0.559156  0.577484\n",
      "1      Random Forest  0.715669  0.606939\n",
      "2            XGBoost  0.625660  0.606636\n",
      "3              Lasso  0.558769  0.578327\n",
      "4              Ridge  0.559138  0.577704\n",
      "5  Gradient Boosting  0.632934  0.605696\n",
      "6           AdaBoost  0.511483  0.504008\n"
     ]
    }
   ],
   "source": [
    "# Best hyperparameters from the GridSearchCV results\n",
    "best_params = {\n",
    "    'Linear Regression': {},\n",
    "    'Random Forest': {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2},\n",
    "    'XGBoost': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100},\n",
    "    'Lasso': {'alpha': 1, 'max_iter': 1000},\n",
    "    'Ridge': {'alpha': 0.01},\n",
    "    'Gradient Boosting': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100},\n",
    "    'AdaBoost': {'learning_rate': 0.1, 'n_estimators': 50}\n",
    "}\n",
    "\n",
    "# Initialize models with the best hyperparameters\n",
    "models_with_best_params = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(max_depth=10, min_samples_leaf=4, min_samples_split=2),\n",
    "    'XGBoost': XGBRegressor(learning_rate=0.1, max_depth=3, n_estimators=100),\n",
    "    'Lasso': Lasso(alpha=1, max_iter=1000),\n",
    "    'Ridge': Ridge(alpha=0.01),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(learning_rate=0.1, max_depth=3, n_estimators=100),\n",
    "    'AdaBoost': AdaBoostRegressor(learning_rate=0.1, n_estimators=50)\n",
    "}\n",
    "\n",
    "# Train and evaluate the models\n",
    "r2_results = []\n",
    "\n",
    "for model_name, model in models_with_best_params.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    train_r2 = r2_score(y_train, model.predict(X_train))\n",
    "    test_r2 = r2_score(y_test, model.predict(X_test))\n",
    "    r2_results.append({'Model': model_name, 'Train R²': train_r2, 'Test R²': test_r2})\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "r2_results_df = pd.DataFrame(r2_results)\n",
    "\n",
    "# Display the results\n",
    "print(r2_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained XGBoost model\n",
    "xgb_model = models_with_best_params['XGBoost']\n",
    "joblib.dump(xgb_model, 'xgboost_model.pkl')\n",
    "\n",
    "print(\"XGBoost model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
