{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importing necessary libraries for data preprocessing, modeling, and evaluation.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCVx\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocessing function to handle missing values, derive new features, and transform data.\n",
    "def preprocess_data(data):\n",
    "    data['Item_Weight'] = data['Item_Weight'].fillna(data.groupby('Item_Type')['Item_Weight'].transform('mean'))\n",
    "    data['Outlet_Size'] = data['Outlet_Size'].fillna(data['Outlet_Size'].mode()[0])\n",
    "\n",
    "    # Feature derivation\n",
    "    data['Outlet_Age'] = 2024 - data['Outlet_Establishment_Year']\n",
    "    data['Price_Per_Unit_Weight'] = data['Item_MRP'] / data['Item_Weight']\n",
    "    \n",
    "    # Simplifying Item_Fat_Content\n",
    "    data['Item_Fat_Content'] = data['Item_Fat_Content'].replace({'LF': 'Low Fat', 'low fat': 'Low Fat', 'reg': 'Regular'})\n",
    "\n",
    "    # Log Transformation for Item Visibility\n",
    "    data['Item_Visibility_Log'] = np.log1p(data['Item_Visibility'])\n",
    "    \n",
    "    # MRP Categorization\n",
    "    min_value = data['Item_MRP'].min()\n",
    "    max_value = data['Item_MRP'].max()\n",
    "    range_value = max_value - min_value\n",
    "    data['MRP_Tier'] = data['Item_MRP'].apply(lambda x: 'Low' if x <= min_value + 0.33 * range_value else\n",
    "                                              'Medium' if x <= min_value + 0.66 * range_value else 'High')\n",
    "\n",
    "    return data\n",
    "\n",
    "# Step 2: Loading data and splitting into training and testing sets.\n",
    "data = pd.read_csv('C:\\\\Users\\\\Kamlesh P Panchal\\\\Documents\\\\Infosys Internship\\\\train_og\\\\Train.csv')\n",
    "training_data, testing_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Preprocessing training and testing datasets.\n",
    "training_data = preprocess_data(training_data)\n",
    "testing_data = preprocess_data(testing_data)\n",
    "\n",
    "# Step 4: Log-transforming the target variable for normalization.\n",
    "training_data['Item_Outlet_Sales'] = np.log1p(training_data['Item_Outlet_Sales'])\n",
    "testing_data['Item_Outlet_Sales'] = np.log1p(testing_data['Item_Outlet_Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Identifier</th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Item_Fat_Content</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_Type</th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Outlet_Identifier</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "      <th>Outlet_Size</th>\n",
       "      <th>Outlet_Location_Type</th>\n",
       "      <th>Outlet_Type</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FDA15</td>\n",
       "      <td>9.3</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.016047</td>\n",
       "      <td>Dairy</td>\n",
       "      <td>249.8092</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 1</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>3735.138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Item_Identifier  Item_Weight Item_Fat_Content  Item_Visibility Item_Type  \\\n",
       "0           FDA15          9.3          Low Fat         0.016047     Dairy   \n",
       "\n",
       "   Item_MRP Outlet_Identifier  Outlet_Establishment_Year Outlet_Size  \\\n",
       "0  249.8092            OUT049                       1999      Medium   \n",
       "\n",
       "  Outlet_Location_Type        Outlet_Type  Item_Outlet_Sales  \n",
       "0               Tier 1  Supermarket Type1           3735.138  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Function for encoding categorical and scaling numerical features.\n",
    "\n",
    "def encode_data(data, is_training=True, encoders=None):\n",
    "    # feature groups\n",
    "    numeric_features = ['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Age', 'Price_Per_Unit_Weight']\n",
    "    ordinal_features = ['Outlet_Size', 'MRP_Tier']\n",
    "    nominal_features = ['Item_Fat_Content', 'Outlet_Location_Type', 'Outlet_Type', 'Item_Type']\n",
    "    label_features = ['Item_Identifier', 'Outlet_Identifier']\n",
    "\n",
    "    # Filtering the columns existing in the dataset\n",
    "    numeric_features = [col for col in numeric_features if col in data.columns]\n",
    "    ordinal_features = [col for col in ordinal_features if col in data.columns]\n",
    "    nominal_features = [col for col in nominal_features if col in data.columns]\n",
    "    label_features = [col for col in label_features if col in data.columns]\n",
    "\n",
    "    if is_training:\n",
    "        # Fitting encoders on training data\n",
    "        encoders = {\n",
    "            'ordinal': OrdinalEncoder().fit(data[ordinal_features]) if ordinal_features else None,\n",
    "            'nominal': OneHotEncoder(sparse_output=False, drop='first').fit(data[nominal_features]) if nominal_features else None,\n",
    "            'label': {col: LabelEncoder().fit(data[col]) for col in label_features},\n",
    "            'scaler': StandardScaler().fit(data[numeric_features]) if numeric_features else None\n",
    "        }\n",
    "\n",
    "    # Applying transformations\n",
    "    if numeric_features:\n",
    "        data[numeric_features] = encoders['scaler'].transform(data[numeric_features])\n",
    "    if ordinal_features:\n",
    "        data[ordinal_features] = encoders['ordinal'].transform(data[ordinal_features])\n",
    "    if nominal_features:\n",
    "        nominal_encoded = encoders['nominal'].transform(data[nominal_features])\n",
    "        nominal_cols = encoders['nominal'].get_feature_names_out(nominal_features)\n",
    "        data = pd.concat([data.reset_index(drop=True), pd.DataFrame(nominal_encoded, columns=nominal_cols)], axis=1)\n",
    "        data.drop(columns=nominal_features, inplace=True)\n",
    "\n",
    "    # Label encode ID columns\n",
    "    for label_feature in label_features:\n",
    "        le = encoders['label'][label_feature]\n",
    "        data[label_feature] = data[label_feature].map(\n",
    "            lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
    "        )\n",
    "        \n",
    "    # Saving encoders to reuse them for new data.\n",
    "    joblib.dump(encoders['ordinal'], 'ordinal_encoder.pkl')\n",
    "    joblib.dump(encoders['nominal'], 'onehot_encoder.pkl')\n",
    "    joblib.dump(encoders['scaler'], 'standard_scaler.pkl')\n",
    "\n",
    "    # Save label encoders for ID features\n",
    "    for label_feature in label_features:\n",
    "        joblib.dump(encoders['label'][label_feature], f'{label_feature}_label_encoder.pkl')\n",
    "\n",
    "    return (data, encoders) if is_training else data\n",
    "\n",
    "# Step 4: Encode training and testing data\n",
    "training_data, encoders = encode_data(training_data, is_training=True)  # Encoding on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading encoders and encoding testing data.   \n",
    "ordinal_encoder = joblib.load('ordinal_encoder.pkl')\n",
    "onehot_encoder = joblib.load('onehot_encoder.pkl')\n",
    "scaler = joblib.load('standard_scaler.pkl')\n",
    "\n",
    "item_identifier_encoder = joblib.load('Item_Identifier_label_encoder.pkl')\n",
    "outlet_identifier_encoder = joblib.load('Outlet_Identifier_label_encoder.pkl')\n",
    "\n",
    "testing_data = encode_data(testing_data, is_training=False, encoders=encoders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Identifier                      int32\n",
       "Item_Weight                        float64\n",
       "Item_Visibility                    float64\n",
       "Item_MRP                           float64\n",
       "Outlet_Identifier                    int32\n",
       "Outlet_Establishment_Year            int64\n",
       "Outlet_Size                        float64\n",
       "Item_Outlet_Sales                  float64\n",
       "Outlet_Age                         float64\n",
       "Price_Per_Unit_Weight              float64\n",
       "Item_Visibility_Log                float64\n",
       "MRP_Tier                           float64\n",
       "Item_Fat_Content_Regular           float64\n",
       "Outlet_Location_Type_Tier 2        float64\n",
       "Outlet_Location_Type_Tier 3        float64\n",
       "Outlet_Type_Supermarket Type1      float64\n",
       "Outlet_Type_Supermarket Type2      float64\n",
       "Outlet_Type_Supermarket Type3      float64\n",
       "Item_Type_Breads                   float64\n",
       "Item_Type_Breakfast                float64\n",
       "Item_Type_Canned                   float64\n",
       "Item_Type_Dairy                    float64\n",
       "Item_Type_Frozen Foods             float64\n",
       "Item_Type_Fruits and Vegetables    float64\n",
       "Item_Type_Hard Drinks              float64\n",
       "Item_Type_Health and Hygiene       float64\n",
       "Item_Type_Household                float64\n",
       "Item_Type_Meat                     float64\n",
       "Item_Type_Others                   float64\n",
       "Item_Type_Seafood                  float64\n",
       "Item_Type_Snack Foods              float64\n",
       "Item_Type_Soft Drinks              float64\n",
       "Item_Type_Starchy Foods            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model  Train R²   Test R²\n",
      "0  Linear Regression  0.727077  0.736358\n",
      "1      Random Forest  0.958539  0.719032\n",
      "2            XGBoost  0.907070  0.702246\n",
      "3              Lasso  0.000041 -0.003547\n",
      "4              Ridge  0.726909  0.736102\n",
      "5  Gradient Boosting  0.756606  0.741898\n",
      "6           AdaBoost  0.576584  0.589278\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Splitting datasets into features (X) and target variable (y).\n",
    "\n",
    "X_train = training_data.drop('Item_Outlet_Sales', axis=1)\n",
    "y_train = training_data['Item_Outlet_Sales']\n",
    "X_test = testing_data.drop('Item_Outlet_Sales', axis=1)\n",
    "y_test = testing_data['Item_Outlet_Sales']\n",
    "\n",
    "# Step 7: Training models and evaluating performance.\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'XGBoost': XGBRegressor(),\n",
    "    'Lasso': Lasso(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(),\n",
    "    'AdaBoost': AdaBoostRegressor()\n",
    "}\n",
    "\n",
    "results = []\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    train_r2 = r2_score(y_train, model.predict(X_train))\n",
    "    test_r2 = r2_score(y_test, model.predict(X_test))\n",
    "    results.append({'Model': model_name, 'Train R²': train_r2, 'Test R²': test_r2})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear Regression...\n",
      "Training Random Forest...\n",
      "Training XGBoost...\n",
      "Training Lasso...\n",
      "Training Ridge...\n",
      "Training Gradient Boosting...\n",
      "Training AdaBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kamlesh P Panchal\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model                                        Best Params  \\\n",
      "0  Linear Regression                                                 {}   \n",
      "1      Random Forest  {'n_estimators': 100, 'min_samples_split': 2, ...   \n",
      "2            XGBoost  {'subsample': 0.9, 'n_estimators': 100, 'max_d...   \n",
      "3              Lasso                                     {'alpha': 0.1}   \n",
      "4              Ridge                                     {'alpha': 0.1}   \n",
      "5  Gradient Boosting  {'subsample': 0.9, 'n_estimators': 100, 'max_d...   \n",
      "6           AdaBoost        {'n_estimators': 100, 'learning_rate': 0.1}   \n",
      "\n",
      "   Train R²   Test R²  \n",
      "0  0.727077  0.736358  \n",
      "1  0.805910  0.738656  \n",
      "2  0.754794  0.744645  \n",
      "3  0.380920  0.381786  \n",
      "4  0.726965  0.736347  \n",
      "5  0.744495  0.740051  \n",
      "6  0.649401  0.657255  \n"
     ]
    }
   ],
   "source": [
    "# Step 8: Hyperparameter tuning using GridSearchCV or RandomizedSearchCV.\n",
    "\n",
    "param_grids = {\n",
    "    'Linear Regression': {},\n",
    "    'Random Forest': {'n_estimators': [100, 200, 300], 'max_depth': [10, 20, 30], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]},\n",
    "    'XGBoost': {'n_estimators': [100, 200, 300], 'learning_rate': [0.01, 0.1, 0.3], 'max_depth': [3, 5, 7], 'subsample': [0.8, 0.9, 1.0]},\n",
    "    'Lasso': {'alpha': [0.1, 1, 10, 100]},\n",
    "    'Ridge': {'alpha': [0.1, 1, 10, 100]},\n",
    "    'Gradient Boosting': {'n_estimators': [100, 200], 'learning_rate': [0.01, 0.05, 0.1], 'max_depth': [3, 5, 7], 'subsample': [0.8, 0.9, 1.0]},\n",
    "    'AdaBoost': {'n_estimators': [50, 100, 150], 'learning_rate': [0.01, 0.1, 1.0]}\n",
    "}\n",
    "\n",
    "results = []\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    if model_name in ['Linear Regression', 'Lasso', 'Ridge']:\n",
    "        search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=3, scoring='r2', n_jobs=-1)\n",
    "    else:\n",
    "        search = RandomizedSearchCV(estimator=model, param_distributions=param_grids[model_name], n_iter=10, cv=3, scoring='r2', random_state=42, n_jobs=-1)\n",
    "        \n",
    "    search.fit(X_train, y_train)\n",
    "    best_model = search.best_estimator_\n",
    "    train_r2 = r2_score(y_train, best_model.predict(X_train))\n",
    "    test_r2 = r2_score(y_test, best_model.predict(X_test))\n",
    "    results.append({'Model': model_name, 'Best Params': search.best_params_, 'Train R²': train_r2, 'Test R²': test_r2})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear Regression with best parameters: {}...\n",
      "Saved Linear Regression model.\n",
      "Training XGBoost with best parameters: {'subsample': 0.9, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.1}...\n",
      "Saved XGBoost model.\n",
      "Training Ridge with best parameters: {'alpha': 0.1}...\n",
      "Saved Ridge model.\n",
      "Training Gradient Boosting with best parameters: {'subsample': 0.9, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1}...\n",
      "Saved Gradient Boosting model.\n",
      "Training AdaBoost with best parameters: {'n_estimators': 100, 'learning_rate': 0.1}...\n",
      "Saved AdaBoost model.\n",
      "               Model  Train R²   Test R²\n",
      "0  Linear Regression  0.727077  0.736358\n",
      "1            XGBoost  0.754794  0.744645\n",
      "2              Ridge  0.726965  0.736347\n",
      "3  Gradient Boosting  0.800216  0.737493\n",
      "4           AdaBoost  0.650757  0.659912\n"
     ]
    }
   ],
   "source": [
    "# Extracting best parameters from the results and retraining the models\n",
    "best_params = {\n",
    "    \"Linear Regression\": {},\n",
    "    \"XGBoost\": {'subsample': 0.9, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.1},\n",
    "    \"Ridge\": {'alpha': 0.1},\n",
    "    \"Gradient Boosting\": {'subsample': 0.9, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1},\n",
    "    \"AdaBoost\": {'n_estimators': 100, 'learning_rate': 0.1}\n",
    "}\n",
    "\n",
    "# Models to be trained\n",
    "model_classes = {\n",
    "    \"Linear Regression\": LinearRegression,\n",
    "    \"XGBoost\": XGBRegressor,\n",
    "    \"Ridge\": Ridge,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor,\n",
    "    \"AdaBoost\": AdaBoostRegressor\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "for model_name, params in best_params.items():\n",
    "    print(f\"Training {model_name} with best parameters: {params}...\")\n",
    "    model = model_classes[model_name](**params)  # Instantiate model with best parameters\n",
    "    model.fit(X_train, y_train)  # Train the model\n",
    "    trained_models[model_name] = model  # Store the trained model\n",
    "\n",
    "    # Save the trained model for future use\n",
    "    joblib.dump(model, f\"{model_name.lower().replace(' ', '_')}_model.pkl\")\n",
    "    print(f\"Saved {model_name} model.\")\n",
    "\n",
    "# Evaluate the retrained models\n",
    "evaluation_results = []\n",
    "for model_name, model in trained_models.items():\n",
    "    train_r2 = r2_score(y_train, model.predict(X_train))\n",
    "    test_r2 = r2_score(y_test, model.predict(X_test))\n",
    "    evaluation_results.append({\"Model\": model_name, \"Train R²\": train_r2, \"Test R²\": test_r2})\n",
    "\n",
    "evaluation_results_df = pd.DataFrame(evaluation_results)\n",
    "print(evaluation_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
