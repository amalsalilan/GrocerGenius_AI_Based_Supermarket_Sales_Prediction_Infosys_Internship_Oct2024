{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfc0e67c",
   "metadata": {},
   "source": [
    "## 1. Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8fc6a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer, OneHotEncoder, OrdinalEncoder\n",
    "import sklearn\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from joblib import dump\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b369898",
   "metadata": {},
   "source": [
    "## 2. Custom Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "730025b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.standard_scaler = StandardScaler()\n",
    "        self.yeo_johnson = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "        self.one_hot_encoder = None\n",
    "        self.ordinal_encoder = None\n",
    "        self.loo_encodings = {}\n",
    "        self.median_visibility = None\n",
    "        self.item_weight_medians = {}\n",
    "        self.outlet_size_modes = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Handle missing Item_Weight\n",
    "        self.item_weight_medians = X.groupby('Item_Type')['Item_Weight'].median().to_dict()\n",
    "\n",
    "        # Handle missing Outlet_Size\n",
    "        self.outlet_size_modes = X.groupby('Outlet_Type')['Outlet_Size'].agg(\n",
    "            lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown'\n",
    "        ).to_dict()\n",
    "\n",
    "        # Replace zero visibility with median visibility\n",
    "        non_zero_visibility = X.loc[X['Item_Visibility'] > 0, 'Item_Visibility']\n",
    "        self.median_visibility = non_zero_visibility.median()\n",
    "\n",
    "        # Fit scalers for numerical columns\n",
    "        self.scaler.fit(X[['Item_MRP']])\n",
    "        self.standard_scaler.fit(X[['Item_Weight', 'Item_Visibility']])\n",
    "\n",
    "        # Fit Yeo-Johnson for skewed features\n",
    "        skewed_columns = ['Item_Weight', 'Item_Visibility', 'Item_MRP']\n",
    "        self.yeo_johnson.fit(X[skewed_columns])\n",
    "\n",
    "        # Fit one-hot encoder for low-cardinality categorical features\n",
    "        low_cardinality_features = ['Outlet_Type']\n",
    "        self.one_hot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        self.one_hot_encoder.fit(X[low_cardinality_features])\n",
    "\n",
    "        # Fit ordinal encoder for features with hierarchy\n",
    "        ordinal_features = ['Outlet_Size', 'Outlet_Location_Type']\n",
    "        self.ordinal_encoder = OrdinalEncoder(categories=[\n",
    "            ['Small', 'Medium', 'High'],  # Outlet_Size\n",
    "            ['Tier 3', 'Tier 2', 'Tier 1']  # Outlet_Location_Type\n",
    "        ], handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        self.ordinal_encoder.fit(X[ordinal_features])\n",
    "\n",
    "        # Calculate LOO encoding for high cardinality features, excluding 'Item_Type'\n",
    "        if y is not None:\n",
    "            y = pd.Series(y, index=X.index)\n",
    "            high_cardinality_features = ['Item_Identifier', 'Outlet_Identifier']\n",
    "            for feature in high_cardinality_features:\n",
    "                loo_encoding = X.groupby(feature).apply(lambda group: y.loc[group.index].mean()).to_dict()\n",
    "                self.loo_encodings[feature] = loo_encoding\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "\n",
    "        # Check required columns\n",
    "        required_columns = ['Item_Type', 'Item_Weight', 'Item_Visibility', 'Item_MRP', \n",
    "                            'Outlet_Type', 'Outlet_Location_Type', 'Outlet_Size']\n",
    "        for col in required_columns:\n",
    "            if col not in X.columns:\n",
    "                raise KeyError(f\"The column '{col}' is missing from the input data.\")\n",
    "\n",
    "        # Handle missing values and preprocessing\n",
    "        X['Item_Weight'] = X['Item_Weight'].fillna(X['Item_Type'].map(self.item_weight_medians))\n",
    "        X['Outlet_Size'] = X['Outlet_Size'].fillna(X['Outlet_Type'].map(self.outlet_size_modes))\n",
    "        X['Item_Visibility'] = X['Item_Visibility'].replace(0, self.median_visibility)\n",
    "\n",
    "        # Normalize and map Item_Fat_Content\n",
    "        X['Item_Fat_Content'] = X['Item_Fat_Content'].str.strip().str.lower()\n",
    "        fat_content_map = {'low fat': 'Low Fat', 'lf': 'Low Fat', 'regular': 'Regular', 'reg': 'Regular'}\n",
    "        X['Item_Fat_Content'] = X['Item_Fat_Content'].map(fat_content_map)\n",
    "\n",
    "        # One-hot encode Item_Fat_Content\n",
    "        one_hot = pd.get_dummies(X['Item_Fat_Content'], prefix='Item_Fat_Content')\n",
    "        X = pd.concat([X.drop(columns=['Item_Fat_Content']), one_hot], axis=1)\n",
    "\n",
    "        # Outlier capping\n",
    "        continuous_columns = ['Item_Weight', 'Item_Visibility', 'Item_MRP']\n",
    "        z_threshold = 3\n",
    "        for col in continuous_columns:\n",
    "            upper_bound = X[col].mean() + z_threshold * X[col].std()\n",
    "            lower_bound = X[col].mean() - z_threshold * X[col].std()\n",
    "            X[col] = np.clip(X[col], lower_bound, upper_bound)\n",
    "\n",
    "        # Apply transformations\n",
    "        skewed_columns = ['Item_Weight', 'Item_Visibility', 'Item_MRP']\n",
    "        X[skewed_columns] = self.yeo_johnson.transform(X[skewed_columns])\n",
    "        X[['Item_Weight', 'Item_Visibility']] = self.standard_scaler.transform(X[['Item_Weight', 'Item_Visibility']])\n",
    "        X[['Item_MRP']] = self.scaler.transform(X[['Item_MRP']])\n",
    "\n",
    "        # Encode Outlet_Type\n",
    "        low_cardinality_features = ['Outlet_Type']\n",
    "        encoded_features = self.one_hot_encoder.transform(X[low_cardinality_features])\n",
    "        encoded_features = pd.DataFrame(\n",
    "            encoded_features,\n",
    "            columns=self.one_hot_encoder.get_feature_names_out(low_cardinality_features),\n",
    "            index=X.index\n",
    "        )\n",
    "        X = pd.concat([X, encoded_features], axis=1).drop(columns=low_cardinality_features)\n",
    "\n",
    "        # Encode hierarchical features\n",
    "        ordinal_features = ['Outlet_Size', 'Outlet_Location_Type']\n",
    "        X[ordinal_features] = self.ordinal_encoder.transform(X[ordinal_features])\n",
    "\n",
    "        # One-hot encode 'Item_Type'\n",
    "        one_hot_item_type = pd.get_dummies(X['Item_Type'], prefix='Item_Type')\n",
    "        X = pd.concat([X.drop(columns=['Item_Type']), one_hot_item_type], axis=1)\n",
    "\n",
    "        # LOO encoding for high cardinality, excluding 'Item_Type'\n",
    "        high_cardinality_features = ['Outlet_Identifier']\n",
    "        for feature in high_cardinality_features:\n",
    "            if feature in X.columns:\n",
    "                X[f'{feature}_LOO'] = X[feature].map(self.loo_encodings.get(feature, {})).fillna(0)\n",
    "\n",
    "        X.drop(columns=high_cardinality_features, inplace=True)\n",
    "        \n",
    "\n",
    "        # Add new feature engineering\n",
    "        X['Outlet_Age'] = 2024 - X['Outlet_Establishment_Year']\n",
    "        X['Visibility_Percentage'] = X['Item_Visibility'] / (X['Item_Visibility'].sum() + 1e-5)\n",
    "        X['Price_Per_Weight'] = X['Item_MRP'] / (X['Item_Weight'] + 1e-5)\n",
    "        X['Visibility_to_MRP_Ratio'] = X['Item_Visibility'] / (X['Item_MRP'] + 1e-5)\n",
    "        X['Discount_Potential'] = X['Item_MRP'] / (X['Item_Visibility'] + 1e-5)\n",
    "\n",
    "        # Remove spaces in column names\n",
    "        X.columns = X.columns.str.replace(' ', '_')\n",
    "        X.drop(columns=['Item_Identifier','Outlet_Establishment_Year'], inplace=True)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00018b86",
   "metadata": {},
   "source": [
    "### 1. Handle Missing Values:\n",
    "   * Replace missing Item_Weight with the median grouped by Item_Type.\n",
    "   * Replace missing Outlet_Size with the mode grouped by Outlet_Type.\n",
    "   * Replace zero Item_Visibility with the median of non-zero values.\n",
    "### 2.Scale Features:\n",
    "   * MinMaxScaler for Item_MRP.\n",
    "   * StandardScaler for Item_Weight and Item_Visibility.\n",
    "   * PowerTransformer (Yeo-Johnson) for skew correction.\n",
    "### 3. Encode Features:\n",
    "   * One-Hot Encoding: Low cardinality categorical features.\n",
    "   * Ordinal Encoding: Features with inherent hierarchy, e.g., Outlet_Size, Outlet_Location_Type.\n",
    "### 4. Feature Engineering:\n",
    "#### Add new features:\n",
    "   * Outlet_Age (current year - Outlet_Establishment_Year).\n",
    "   * Visibility_Percentage (Item_Visibility / total visibility).\n",
    "   * Price_Per_Weight (Item_MRP / Item_Weight).\n",
    "   * Visibility_to_MRP_Ratio (Item_Visibility / Item_MRP).\n",
    "   * Discount_Potential (Item_MRP / Item_Visibility).\n",
    "### 5. Outlier Handling:\n",
    "   * Apply a Z-score threshold of 3 for continuous variables.\n",
    "### 6. Leave-One-Out Encoding (LOO):\n",
    "   * Applied to high cardinality features like Item_Identifier and Outlet_Identifier based on the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97778ed",
   "metadata": {},
   "source": [
    "## 3. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ca29ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Downloads\\Infosys Springboard\\Train.csv')\n",
    "target_variable = 'Item_Outlet_Sales'\n",
    "X = data.drop(columns=[target_variable])\n",
    "y = data[target_variable]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743c1c42",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "   * Load the dataset.\n",
    "   * Separate features (X) and target (y).\n",
    "   * Split into training and testing datasets (80%-20% split)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d4df4f",
   "metadata": {},
   "source": [
    "## 4. Fit and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77e29c9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data missing values:\n",
      " Item_Weight                        0\n",
      "Item_Visibility                    0\n",
      "Item_MRP                           0\n",
      "Outlet_Size                        0\n",
      "Outlet_Location_Type               0\n",
      "Item_Fat_Content_Low_Fat           0\n",
      "Item_Fat_Content_Regular           0\n",
      "Outlet_Type_Grocery_Store          0\n",
      "Outlet_Type_Supermarket_Type1      0\n",
      "Outlet_Type_Supermarket_Type2      0\n",
      "Outlet_Type_Supermarket_Type3      0\n",
      "Item_Type_Baking_Goods             0\n",
      "Item_Type_Breads                   0\n",
      "Item_Type_Breakfast                0\n",
      "Item_Type_Canned                   0\n",
      "Item_Type_Dairy                    0\n",
      "Item_Type_Frozen_Foods             0\n",
      "Item_Type_Fruits_and_Vegetables    0\n",
      "Item_Type_Hard_Drinks              0\n",
      "Item_Type_Health_and_Hygiene       0\n",
      "Item_Type_Household                0\n",
      "Item_Type_Meat                     0\n",
      "Item_Type_Others                   0\n",
      "Item_Type_Seafood                  0\n",
      "Item_Type_Snack_Foods              0\n",
      "Item_Type_Soft_Drinks              0\n",
      "Item_Type_Starchy_Foods            0\n",
      "Outlet_Identifier_LOO              0\n",
      "Outlet_Age                         0\n",
      "Visibility_Percentage              0\n",
      "Price_Per_Weight                   0\n",
      "Visibility_to_MRP_Ratio            0\n",
      "Discount_Potential                 0\n",
      "dtype: int64\n",
      "Testing data missing values:\n",
      " Item_Weight                        0\n",
      "Item_Visibility                    0\n",
      "Item_MRP                           0\n",
      "Outlet_Size                        0\n",
      "Outlet_Location_Type               0\n",
      "Item_Fat_Content_Low_Fat           0\n",
      "Item_Fat_Content_Regular           0\n",
      "Outlet_Type_Grocery_Store          0\n",
      "Outlet_Type_Supermarket_Type1      0\n",
      "Outlet_Type_Supermarket_Type2      0\n",
      "Outlet_Type_Supermarket_Type3      0\n",
      "Item_Type_Baking_Goods             0\n",
      "Item_Type_Breads                   0\n",
      "Item_Type_Breakfast                0\n",
      "Item_Type_Canned                   0\n",
      "Item_Type_Dairy                    0\n",
      "Item_Type_Frozen_Foods             0\n",
      "Item_Type_Fruits_and_Vegetables    0\n",
      "Item_Type_Hard_Drinks              0\n",
      "Item_Type_Health_and_Hygiene       0\n",
      "Item_Type_Household                0\n",
      "Item_Type_Meat                     0\n",
      "Item_Type_Others                   0\n",
      "Item_Type_Seafood                  0\n",
      "Item_Type_Snack_Foods              0\n",
      "Item_Type_Soft_Drinks              0\n",
      "Item_Type_Starchy_Foods            0\n",
      "Outlet_Identifier_LOO              0\n",
      "Outlet_Age                         0\n",
      "Visibility_Percentage              0\n",
      "Price_Per_Weight                   0\n",
      "Visibility_to_MRP_Ratio            0\n",
      "Discount_Potential                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "processor = DataProcessor()\n",
    "X_train = processor.fit_transform(X_train, y_train)\n",
    "X_test = processor.transform(X_test)\n",
    "\n",
    "print(\"Training data missing values:\\n\", X_train.isnull().sum())\n",
    "print(\"Testing data missing values:\\n\", X_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae43becf",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "   * Fit: Learns transformations from training data (e.g., missing value imputations, scalers).\n",
    "   * Transform: Applies transformations to both training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7207fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['median_visibility.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b361387",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b615a822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear Regression...\n",
      "Training Random Forest...\n",
      "Training Gradient Boosting...\n",
      "Training XGBoost...\n",
      "Training MLP Regressor...\n",
      "\n",
      "Model Performance (R² Scores):\n",
      "               Model  Training R²  Testing R²\n",
      "0  Linear Regression     0.562106    0.578638\n",
      "1      Random Forest     0.723045    0.606711\n",
      "2  Gradient Boosting     0.670230    0.597786\n",
      "3            XGBoost     0.813836    0.572002\n",
      "4      MLP Regressor     0.233134    0.261973\n"
     ]
    }
   ],
   "source": [
    "# Define models with improved hyperparameters\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=200, learning_rate=0.1, random_state=42, verbosity=0),\n",
    "    'MLP Regressor': MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam',\n",
    "                                   max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "r2_scores = {'Model': [], 'Training R²': [], 'Testing R²': []}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    # Convert X_train to numpy if necessary for XGBoost\n",
    "    if model_name == 'XGBoost':\n",
    "        X_train_data, X_test_data = X_train.to_numpy(), X_test.to_numpy()\n",
    "    else:\n",
    "        X_train_data, X_test_data = X_train, X_test\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train_data, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    train_preds = model.predict(X_train_data)\n",
    "    test_preds = model.predict(X_test_data)\n",
    "    \n",
    "    # R² Scores\n",
    "    r2_scores['Model'].append(model_name)\n",
    "    r2_scores['Training R²'].append(r2_score(y_train, train_preds))\n",
    "    r2_scores['Testing R²'].append(r2_score(y_test, test_preds))\n",
    "\n",
    "# Create a DataFrame for R² scores\n",
    "r2_scores_df = pd.DataFrame(r2_scores)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nModel Performance (R² Scores):\")\n",
    "print(r2_scores_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93463299",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "   * Train each model on the training data.\n",
    "   * Evaluate both training and testing R² scores to check for underfitting/overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4354a43b",
   "metadata": {},
   "source": [
    "## 6. XGBoost Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c482e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning XGBoost...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best Params for XGBoost: {'subsample': 0.8, 'reg_lambda': 10, 'reg_alpha': 5, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.05, 'gamma': 1, 'colsample_bytree': 0.8}\n",
      "Best CV R² for XGBoost: 0.5971\n",
      "XGBoost Training R²: 0.6283\n",
      "XGBoost Testing R²: 0.6196\n"
     ]
    }
   ],
   "source": [
    "# Define XGBoost hyperparameter grid\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'gamma': [0, 1, 5],\n",
    "    'reg_alpha': [0, 1, 5],\n",
    "    'reg_lambda': [1, 5, 10],\n",
    "}\n",
    "\n",
    "# Initialize XGBRegressor\n",
    "xgb = XGBRegressor(random_state=42, verbosity=0)\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=xgb_param_grid,\n",
    "    n_iter=50,\n",
    "    scoring='r2',\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"Tuning XGBoost...\")\n",
    "xgb_search.fit(X_train, y_train)\n",
    "\n",
    "# Best XGBoost model\n",
    "best_xgb = xgb_search.best_estimator_\n",
    "print(f\"Best Params for XGBoost: {xgb_search.best_params_}\")\n",
    "print(f\"Best CV R² for XGBoost: {xgb_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate XGBoost on train and test sets\n",
    "xgb_train_r2 = best_xgb.score(X_train, y_train)\n",
    "xgb_test_r2 = best_xgb.score(X_test, y_test)\n",
    "print(f\"XGBoost Training R²: {xgb_train_r2:.4f}\")\n",
    "print(f\"XGBoost Testing R²: {xgb_test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90461b87",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "   * Performed randomized hyperparameter search for XGBoost using RandomizedSearchCV.\n",
    "   * Optimized parameters like n_estimators, learning_rate, max_depth, subsample, and regularization terms (reg_alpha, reg_lambda).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b49ce75",
   "metadata": {},
   "source": [
    "## 7. Retrain Best XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63c6dd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining XGBoost with best parameters...\n",
      "XGBoost Retrained Training R²: 0.6298\n",
      "XGBoost Retrained Testing R²: 0.6196\n",
      "XGBoost model saved to best_xgb_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Ensure X_train and X_test are NumPy arrays\n",
    "X_train_np = X_train.to_numpy() if isinstance(X_train, pd.DataFrame) else X_train\n",
    "X_test_np = X_test.to_numpy() if isinstance(X_test, pd.DataFrame) else X_test\n",
    "\n",
    "# Ensure y_train and y_test are NumPy arrays\n",
    "y_train_np = y_train.to_numpy() if isinstance(y_train, pd.Series) else y_train\n",
    "y_test_np = y_test.to_numpy() if isinstance(y_test, pd.Series) else y_test\n",
    "\n",
    "# Best Parameters from Hyperparameter Tuning\n",
    "best_params = {\n",
    "    'subsample': 1.0,\n",
    "    'reg_lambda': 5,\n",
    "    'reg_alpha': 0,\n",
    "    'n_estimators': 500,\n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.01,\n",
    "    'gamma': 5,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'verbosity': 0,\n",
    "    'enable_categorical': True  # Ensure proper handling if using categorical data\n",
    "}\n",
    "\n",
    "# Retrain the XGBoost Model\n",
    "print(\"Retraining XGBoost with best parameters...\")\n",
    "best_xgb = XGBRegressor(**best_params)\n",
    "best_xgb.fit(X_train_np, y_train_np)\n",
    "\n",
    "# Evaluate on Training and Test Sets\n",
    "xgb_train_r2 = best_xgb.score(X_train_np, y_train_np)\n",
    "xgb_test_r2 = best_xgb.score(X_test_np, y_test_np)\n",
    "\n",
    "print(f\"XGBoost Retrained Training R²: {xgb_train_r2:.4f}\")\n",
    "print(f\"XGBoost Retrained Testing R²: {xgb_test_r2:.4f}\")\n",
    "\n",
    "# Save the Retrained Model\n",
    "model_path = 'best_xgb_model.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(best_xgb, f)\n",
    "\n",
    "print(f\"XGBoost model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34999f7e",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "   * Retrain XGBoost with the best parameters obtained from tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc0c7a4",
   "metadata": {},
   "source": [
    "## 8. Save Preprocessors and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27c2f0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['median_visibility.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save each part separately after fitting\n",
    "dump(processor.scaler, 'scaler.pkl')\n",
    "dump(processor.standard_scaler, 'standard_scaler.pkl')\n",
    "dump(processor.yeo_johnson, 'yeo_johnson.pkl')\n",
    "dump(processor.one_hot_encoder, 'one_hot_encoder.pkl')\n",
    "dump(processor.ordinal_encoder, 'ordinal_encoder.pkl')\n",
    "dump(processor.loo_encodings, 'loo_encodings.pkl')\n",
    "dump(processor.item_weight_medians, 'item_weight_medians.pkl')\n",
    "dump(processor.outlet_size_modes, 'outlet_size_modes.pkl')\n",
    "dump(processor.median_visibility, 'median_visibility.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
